# LLM Configuration Options (choose one)

# Option 1: LlamaCpp (Recommended for corporate environments)
# Download a GGUF model file and specify the path below
MODEL=llama-cpp
# LLAMA_CPP_MODEL_PATH=/path/to/your/model.gguf

# Option 2: Ollama (if network allows downloads)
# MODEL=llama3.1:8b
# OLLAMA_HOST=http://localhost:11434

# Option 3: Mock LLM (for testing without any model)
# MODEL=mock

# AWS Configuration (optional - only needed if you want to audit AWS resources)
# You can leave these empty if you just want to test the system without AWS
AWS_REGION_NAME=us-east-1
# AWS_ACCESS_KEY_ID=your_aws_access_key_here
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here

# Serper API key for research (optional)
# SERPER_API_KEY=your_serper_api_key_here

# Report output configuration
REPORT_BUCKET_NAME=local-reports 